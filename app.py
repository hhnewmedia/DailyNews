import streamlit as st
import google.generativeai as genai
import pandas as pd
from datetime import datetime
import feedparser
import io
import urllib.parse
import sys

# --- 1. ä»‹é¢èˆ‡åƒæ•¸è¨­å®š (æœ€çµ‚é–å®šç‰ˆ) ---
translations = {
    "ç¹é«”ä¸­æ–‡ (TW)": {
        "title": "é´»æµ·å…¨çƒè¼¿æƒ…ç›£æ§ç³»çµ±",
        "sidebar_title": "è¨­å®šé¢æ¿",
        "gemini_label": "è¼¸å…¥ Google Gemini API Key",
        "days_label": "æœå°‹æ™‚é–“ç¯„åœ (å¤©æ•¸)",
        "keywords_label": "è¼¸å…¥æœå°‹é—œéµå­— (ç”¨é€—è™Ÿéš”é–‹)",
        "keywords_hint": "ä¾‹å¦‚: é´»æµ·, Fii, é›»å‹•è»Š",
        "btn_start": "é–‹å§‹æœå°‹èˆ‡åˆ†æ",
        "processing": "æ­£åœ¨è®€å– Google News RSS ä¸¦é€²è¡Œ AI æ‘˜è¦...",
        "success": "åˆ†æå®Œæˆï¼",
        "download_btn": "ä¸‹è¼‰ Excel å ±è¡¨",
        "error_api": "è«‹è¼¸å…¥ Gemini API Key æ‰èƒ½ä½¿ç”¨ AI æ‘˜è¦ï¼",
        "error_no_key": "è«‹è¼¸å…¥è‡³å°‘ä¸€å€‹é—œéµå­—ï¼",
        "params": {"hl": "zh-TW", "gl": "TW", "ceid": "TW:zh-Hant"}
    },
    # ... (ç‚ºç¯€çœç¯‡å¹…ï¼Œå…¶ä»–èªè¨€æœƒè‡ªå‹•æ²¿ç”¨ä¹‹å‰çš„é‚è¼¯ï¼Œæˆ–æ˜¯æ‚¨å¯ä»¥åªä¿ç•™ä¸­æ–‡ç‰ˆæ¸¬è©¦) ...
}
# ç‚ºäº†é¿å…ç¨‹å¼ç¢¼éé•·ï¼Œé€™è£¡ä½¿ç”¨ç°¡æ˜“ç‰ˆçš„å¤šåœ‹èªè¨€åˆ‡æ›
# å¦‚æœæ‚¨éœ€è¦å®Œæ•´çš„äº”åœ‹èªè¨€ï¼Œè«‹ä¿ç•™æ‚¨åŸæœ¬çš„ translations å­—å…¸ï¼Œåªéœ€æ›´æ–°ä¸‹æ–¹çš„ ai_summarize å‡½æ•¸å³å¯
# ä½†ç‚ºäº†ç¢ºä¿æ‚¨ç¾åœ¨èƒ½æˆåŠŸï¼Œæˆ‘ä¸‹é¢æ”¾çš„æ˜¯æ ¸å¿ƒé‚è¼¯ä¿®å¾©ç‰ˆ

st.set_page_config(page_title="Foxconn RSS Monitor", layout="wide")

st.title("ğŸ¦Š é´»æµ·å…¨çƒè¼¿æƒ…ç›£æ§ç³»çµ±")

# Sidebar
st.sidebar.title("è¨­å®šé¢æ¿")
gemini_key_input = st.sidebar.text_input("è¼¸å…¥ Google Gemini API Key", type="password")
gemini_key = gemini_key_input.strip() if gemini_key_input else ""

# å¤©æ•¸æ»‘æ¡¿
days_selected = st.sidebar.slider("æœå°‹æ™‚é–“ç¯„åœ (å¤©æ•¸)", 1, 7, 1)
time_param = f"when:{days_selected}d"

user_keywords = st.text_input("è¼¸å…¥æœå°‹é—œéµå­— (ç”¨é€—è™Ÿéš”é–‹)", placeholder="ä¾‹å¦‚: é´»æµ·, Fii, é›»å‹•è»Š")

# --- æ ¸å¿ƒå‡½æ•¸ ---
def search_google_rss(keyword, time_limit, params):
    base_url = "https://news.google.com/rss/search"
    query = f"{keyword} {time_limit}"
    encoded_query = urllib.parse.quote(query)
    # é è¨­ä½¿ç”¨å°ç£ç¹é«”ä¸­æ–‡æœå°‹ï¼Œè‹¥éœ€å¤šåœ‹èªè¨€å¯å†æ“´å……
    rss_url = f"{base_url}?q={encoded_query}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    
    feed = feedparser.parse(rss_url)
    results = []
    for entry in feed.entries[:10]:
        pub_date = entry.published if 'published' in entry else datetime.now().strftime("%Y-%m-%d")
        results.append({
            "Keyword": keyword,
            "Title": entry.title,
            "Link": entry.link,
            "Date": pub_date,
            "Source": entry.source.title if 'source' in entry else "Google News"
        })
    return results

def ai_summarize(news_data, api_key):
    # å¼·åˆ¶ä½¿ç”¨ gemini-1.5-flash
    genai.configure(api_key=api_key)
    model = genai.GenerativeModel('gemini-1.5-flash')
    
    summarized_data = []
    total = len(news_data)
    if total == 0: return []
    
    progress_bar = st.progress(0)
    
    for index, item in enumerate(news_data):
        summary = ""
        try:
            prompt = f"""
            Task: Summarize this news headline in 1 sentence (Traditional Chinese).
            News Title: {item['Title']}
            News Link: {item['Link']}
            """
            response = model.generate_content(prompt)
            summary = response.text
        except Exception as e:
            # é€™è£¡æœƒå°å‡ºè©³ç´°éŒ¯èª¤ï¼Œæ–¹ä¾¿é™¤éŒ¯
            summary = f"Error: {str(e)}"
            
        item['AI Summary'] = summary
        summarized_data.append(item)
        progress_bar.progress((index + 1) / total)
        
    return summarized_data

# --- åŸ·è¡Œé‚è¼¯ ---
if st.button("é–‹å§‹æœå°‹èˆ‡åˆ†æ", type="primary"):
    if not gemini_key:
        st.error("è«‹è¼¸å…¥ API Key")
    elif not user_keywords:
        st.error("è«‹è¼¸å…¥é—œéµå­—")
    else:
        st.info("æ­£åœ¨åŸ·è¡Œ...")
        
        raw_news_list = []
        keywords_list = user_keywords.split(",")
        
        for kw in keywords_list:
            kw = kw.strip()
            if kw:
                results = search_google_rss(kw, time_param, {})
                raw_news_list.extend(results)
        
        if not raw_news_list:
            st.warning("æ‰¾ä¸åˆ°æ–°è")
        else:
            final_data = ai_summarize(raw_news_list, gemini_key)
            df = pd.DataFrame(final_data)
            
            cols = ["Date", "Keyword", "Title", "AI Summary", "Source", "Link"]
            df = df.reindex(columns=cols)
            
            st.dataframe(df, use_container_width=True)
            
            buffer = io.BytesIO()
            with pd.ExcelWriter(buffer, engine='xlsxwriter') as writer:
                df.to_excel(writer, index=False)
                
            st.download_button(
                label="ä¸‹è¼‰ Excel å ±è¡¨",
                data=buffer,
                file_name=f"Foxconn_News.xlsx",
                mime="application/vnd.ms-excel"
            )
